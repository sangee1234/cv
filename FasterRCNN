- Faster RCNN = RPN + RCNN

RPN: Region Proposal Network
-generate anchor boxes
-classify each anchor box as foreground or background
-learn shape offsets for anchor boxes to fit object

-Anchor point: every point in feature map is anchor point
-For every anchor point, need to generate anchor boxes
-Generate candidate object using aspect ratio and scale, box has image dimensions
-take scale down to feature map as stride, and use combination of scales and ratios to generate boxes
-learn if these objects are foreground or background, learn offsets for foreground to adjust fitting the objects
-above 2 steps are acheived by 2 conv layers on feature map
    1. rpn_cls_score
    2. rpn_bbox pred, using regression
        -we need targets to learn offsets
        -these are generated by comparing anchor boxes with ground truth
        -iou is calculated and difference is used as targets to be learned
-these learned regions are propagated through ROI pooling layer and fc layer.

-Region of interest pooling
Major hurdle in object detectinoo is fixed size input requirement to network for fully connected layers.
All proposals must be converted to fixed shape.
It takes the feature map and ROIs from feature map and uses them to generate fixed size inputs

RPN uses pre-trained CNN model to extract feature map - this becomess 1st layer feature map
then we calculate stride between feature map and original image
using the stride we spilt the image into many tiles, these are used to create anchors
each 1st layer feature map point(anchor point) have a tile mapped
for each tile, can have several boxes of different size and starting (outside image) to be drawn on it
(diagram in https://dongjk.github.io/code/object+detection/keras/2018/05/21/Faster_R-CNN_step_by_step,_Part_I.html)

-remove anchores outside image
-use roi to calculate overlap between anchor and ground

